---
---
@article{li2025efficient,
  title    = {Efficient Best-of-Both-Worlds Algorithms for Contextual Combinatorial Semi-Bandits},
  author   = {Li, Mengmeng and Schneider, Philipp and Aleksić, Jelisaveta and Kuhn, Daniel},
  abstract = {We introduce the first best-of-both-worlds algorithm for contextual combinatorial semi-bandits
that simultaneously guarantees O(√T) regret in the adversarial regime and O(ln T) regret in
the corrupted stochastic regime. Our approach builds on the Follow-the-Regularized-Leader
(FTRL) framework equipped with a Shannon entropy regularizer, yielding a flexible method that
admits efficient implementations. Beyond regret bounds, we tackle the practical bottleneck in
FTRL (or, equivalently, Online Stochastic Mirror Descent) arising from the high-dimensional
projection step encountered in each round of interaction. By leveraging the Karush-Kuhn-Tucker
conditions, we transform the K-dimensional convex projection problem into a single-variable
root-finding problem, dramatically accelerating each round. Empirical evaluations demonstrate
that this combined strategy not only attains the attractive regret bounds of best-of-both-worlds
algorithms but also delivers substantial per-round speed-ups, making it well-suited for large-scale,
real-time applications.},
  selected = {false},
  arxiv    = {2508.18768},
  journal  = {Submitted},
  year     = {2025}
}

@article{li2025towards,
  title    = {Towards Optimal Offline Reinforcement Learning},
  author   = {Li, Mengmeng and Kuhn, Daniel and Sutter, Tobias},
  abstract = {We study offline reinforcement learning problems with a long-run average reward objective. The state-action pairs generated by any fixed behavioral policy thus follow a Markov chain, and the empirical state-action-next-state distribution satisfies a large deviations principle. We use the rate function of this large deviations principle to construct an uncertainty set for the unknown true state-action-next-state distribution. We also construct a distribution shift transformation that maps any distribution in this uncertainty set to a state-action-next-state distribution of the Markov chain generated by a fixed evaluation policy, which may differ from the unknown behavioral policy. We prove that the worst-case average reward of the evaluation policy with respect to all distributions in the shifted uncertainty set provides, in a rigorous statistical sense, the least conservative estimator for the average reward under the unknown true distribution. This guarantee is available even if one has only access to one single trajectory of serially correlated state-action pairs. The emerging robust optimization problem can be viewed as a robust Markov decision process with a non-rectangular uncertainty set. We adapt an efficient policy gradient algorithm to solve this problem. Numerical experiments show that our methods compare favorably against state-of-the-art methods.
},
  selected = {false},
  arxiv    = {2503.12283},
  journal  = {Submitted},
  award = {Second Place, 2025 Dupačová-Prékopa Best Student Paper in Stochastic Programming},
  year     = {2025}
}

@article{li2024optimism,
  title    = {Optimism in the Face of Ambiguity Principle for Multi-Armed Bandits},
  author   = {Li, Mengmeng and Kuhn, Daniel and Ta{\c s}kesen, Bahar},
  arxiv    = {2409.20440},
  abstract = {Follow-The-Regularized-Leader (FTRL) algorithms often enjoy optimal regret for adversarial as well as stochastic bandit problems and allow for a streamlined analysis. Nonetheless, FTRL algorithms require the solution of an optimization problem in every iteration and are thus computationally challenging. In contrast, Follow-The-Perturbed-Leader (FTPL) algorithms achieve computational efficiency by perturbing the estimates of the rewards of the arms, but their regret analysis is cumbersome. We propose a new FTPL algorithm that generates optimal policies for both adversarial and stochastic multi-armed bandits. Like FTRL, our algorithm admits a unified regret analysis, and similar to FTPL, it offers low computational costs. Unlike existing FTPL algorithms that rely on independent additive disturbances governed by a known distribution, we allow for disturbances governed by an ambiguous distribution that is only known to belong to a given set and propose a principle of optimism in the face of ambiguity. Consequently, our framework generalizes existing FTPL algorithms. It also encapsulates a broad range of FTRL methods as special cases, including several optimal ones, which appears to be impossible with current FTPL methods. Finally, we use techniques from discrete choice theory to devise an efficient bisection algorithm for computing the optimistic arm sampling probabilities. This algorithm is up to 10^4 times faster than standard FTRL algorithms that solve an optimization problem in every iteration. Our results not only settle existing conjectures but also provide new insights into the impact of perturbations by mapping FTRL to FTPL.},
  journal  = {Major revision in Operations Research},
  note     = {Extended abstract appeared in WINE 2024},
  year     = {2025}
}

@inproceedings{jongeneel2023large,
  title     = {A Large Deviations Perspective on Policy Gradient Algorithms},
  author    = {Jongeneel, (α-β) Wouter and Kuhn, Daniel and Li, Mengmeng},
  abstract  = {Motivated by policy gradient methods in the context of reinforcement learning, we identify a large deviation rate function for the iterates generated by stochastic gradient descent for possibly non-convex objectives satisfying a Polyak-Łojasiewicz condition. Leveraging the contraction principle from large deviations theory, we illustrate the potential of this result by showing how convergence properties of policy gradient with a softmax parametrization and an entropy regularized objective can be naturally extended to a wide spectrum of other policy parametrizations.},
  html      = {https://proceedings.mlr.press/v242/jongeneel24a/jongeneel24a.pdf},
  selected  = {false},
  booktitle = {Learning for Dynamics and Control Conference (L4DC)},
  year      = {2024}
}


@article{li2023policy,
  title    = {Policy Gradient Algorithms for Robust {MDP}s with Non-Rectangular Uncertainty Sets},
  author   = {Li, Mengmeng and Kuhn, Daniel and Sutter, Tobias},
  abstract = {We propose policy gradient algorithms for robust infinite-horizon Markov decision processes (MDPs) with non-rectangular uncertainty sets, thereby addressing an open challenge in the robust MDP literature. Indeed, uncertainty sets that display statistical optimality properties and make optimal use of limited data often fail to be rectangular. Unfortunately, the corresponding robust MDPs cannot be solved with dynamic programming techniques and are in fact provably intractable. We first present a randomized projected Langevin dynamics algorithm that solves the robust policy evaluation problem to global optimality but is inefficient. We also propose a deterministic policy gradient method that is efficient but solves the robust policy evaluation problem only approximately, and we prove that the approximation error scales with a new measure of non-rectangularity of the uncertainty set. Finally, we describe an actor-critic algorithm that finds an ϵ-optimal solution for the robust policy improvement problem in O(1/ϵ^4) iterations. We thus present the first complete solution scheme for robust MDPs with non-rectangular uncertainty sets offering global optimality guarantees. Numerical experiments show that our algorithms compare favorably against state-of-the-art methods.},
  selected = {false},
  arxiv    = {2305.19004},
  journal  = {Forthcoming, SIAM Journal on Optimization},
  year     = {2025}
}

@inproceedings{li2021distributionally,
  title     = {Distributionally Robust Optimization with {M}arkovian Data},
  author    = {Li, Mengmeng and Sutter, Tobias and Kuhn, Daniel},
  abstract  = {We study a stochastic program where the probability distribution of the uncertain problem parameters is unknown and only indirectly observed via finitely many correlated samples generated by an unknown Markov chain with d states. We propose a data-driven distributionally robust optimization model to estimate the problem’s objective function and optimal solution. By leveraging results from large deviations theory, we derive statistical guarantees on the quality of these estimators. The underlying worst-case expectation problem is nonconvex and involves $O(d^2)$ decision variables. Thus, it cannot be solved efficiently for large d. By exploiting the structure of this problem, we devise a customized Frank-Wolfe algorithm with convex direction-finding subproblems of size $O(d)$. We prove that this algorithm finds a stationary point efficiently under mild conditions. The efficiency of the method is predicated on a dimensionality reduction enabled by a dual reformulation. Numerical experiments indicate that our approach has better computational and statistical properties than the state-of-the-art methods.},
  selected  = {false},
  html      = {https://proceedings.mlr.press/v139/li21t.html},
  year      = {2021},
  booktitle = {International Conference on Machine Learning (ICML)}
}

