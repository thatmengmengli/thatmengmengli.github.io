---
---

@string{icml = {International Conference on Machine Learning,}}


@article{li2024optimism,
  title    = {Optimism in the Face of Ambiguity Principle for Multi-Armed Bandits},
  author   = {Li, Mengmeng and Kuhn, Daniel and Ta{\c s}kesen, Bahar},
  arxiv    = {2409.20440},
  abstract = {Follow-The-Regularized-Leader (FTRL) algorithms often enjoy optimal regret for adversarial as well as stochastic bandit problems and allow for a streamlined analysis. Nonetheless, FTRL algorithms require the solution of an optimization problem in every iteration and are thus computationally challenging. In contrast, Follow-The-Perturbed-Leader (FTPL) algorithms achieve computational efficiency by perturbing the estimates of the rewards of the arms, but their regret analysis is cumbersome. We propose a new FTPL algorithm that generates optimal policies for both adversarial and stochastic multi-armed bandits. Like FTRL, our algorithm admits a unified regret analysis, and similar to FTPL, it offers low computational costs. Unlike existing FTPL algorithms that rely on independent additive disturbances governed by a known distribution, we allow for disturbances governed by an ambiguous distribution that is only known to belong to a given set and propose a principle of optimism in the face of ambiguity. Consequently, our framework generalizes existing FTPL algorithms. It also encapsulates a broad range of FTRL methods as special cases, including several optimal ones, which appears to be impossible with current FTPL methods. Finally, we use techniques from discrete choice theory to devise an efficient bisection algorithm for computing the optimistic arm sampling probabilities. This algorithm is up to 10^4 times faster than standard FTRL algorithms that solve an optimization problem in every iteration. Our results not only settle existing conjectures but also provide new insights into the impact of perturbations by mapping FTRL to FTPL.},
  journal  = {Submitted},
  note     = {Extended abstract appeared in WINE 2024},
  year     = {2025}
}

@inproceedings{jongeneel2023large,
  title     = {A Large Deviations Perspective on Policy Gradient Algorithms},
  author    = {Jongeneel, (α-β) Wouter and Kuhn, Daniel and Li, Mengmeng},
  abstract  = {Motivated by policy gradient methods in the context of reinforcement learning, we identify a large deviation rate function for the iterates generated by stochastic gradient descent for possibly non-convex objectives satisfying a Polyak-Łojasiewicz condition. Leveraging the contraction principle from large deviations theory, we illustrate the potential of this result by showing how convergence properties of policy gradient with a softmax parametrization and an entropy regularized objective can be naturally extended to a wide spectrum of other policy parametrizations.},
  html      = {https://proceedings.mlr.press/v242/jongeneel24a/jongeneel24a.pdf},
  selected  = {false},
  booktitle = {Learning for Dynamics and Control Conference (L4DC)},
  year      = {2024}
}


@article{li2023policy,
  title    = {Policy Gradient Algorithms for Robust {MDP}s with Non-Rectangular Uncertainty Sets},
  author   = {Li, Mengmeng and Kuhn, Daniel and Sutter, Tobias},
  abstract = {We propose policy gradient algorithms for robust infinite-horizon Markov decision processes (MDPs) with non-rectangular uncertainty sets, thereby addressing an open challenge in the robust MDP literature. Indeed, uncertainty sets that display statistical optimality properties and make optimal use of limited data often fail to be rectangular. Unfortunately, the corresponding robust MDPs cannot be solved with dynamic programming techniques and are in fact provably intractable. We first present a randomized projected Langevin dynamics algorithm that solves the robust policy evaluation problem to global optimality but is inefficient. We also propose a deterministic policy gradient method that is efficient but solves the robust policy evaluation problem only approximately, and we prove that the approximation error scales with a new measure of non-rectangularity of the uncertainty set. Finally, we describe an actor-critic algorithm that finds an ϵ-optimal solution for the robust policy improvement problem in O(1/ϵ^4) iterations. We thus present the first complete solution scheme for robust MDPs with non-rectangular uncertainty sets offering global optimality guarantees. Numerical experiments show that our algorithms compare favorably against state-of-the-art methods.},
  selected = {false},
  arxiv    = {2305.19004},
  journal  = {Under Revision},
  year     = {2023}
}

@inproceedings{li2021distributionally,
  title     = {Distributionally Robust Optimization with {M}arkovian Data},
  author    = {Li, Mengmeng and Sutter, Tobias and Kuhn, Daniel},
  abstract  = {We study a stochastic program where the probability distribution of the uncertain problem parameters is unknown and only indirectly observed via finitely many correlated samples generated by an unknown Markov chain with d states. We propose a data-driven distributionally robust optimization model to estimate the problem’s objective function and optimal solution. By leveraging results from large deviations theory, we derive statistical guarantees on the quality of these estimators. The underlying worst-case expectation problem is nonconvex and involves $O(d^2)$ decision variables. Thus, it cannot be solved efficiently for large d. By exploiting the structure of this problem, we devise a customized Frank-Wolfe algorithm with convex direction-finding subproblems of size $O(d)$. We prove that this algorithm finds a stationary point efficiently under mild conditions. The efficiency of the method is predicated on a dimensionality reduction enabled by a dual reformulation. Numerical experiments indicate that our approach has better computational and statistical properties than the state-of-the-art methods.},
  selected  = {false},
  html      = {https://proceedings.mlr.press/v139/li21t.html},
  year      = {2021},
  booktitle = {International Conference on Machine Learning (ICML)}
}

